import streamlit as st
import time
import os
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ë° CSS ---
st.set_page_config(
    page_title="ì •ì±… íë ˆì´í„°",
    page_icon="ğŸ¤–",
    layout="wide"
)

# [ê°œì„ ] ì„¸ë ¨ëœ UIë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+KR:wght@300;400;500;600;700&display=swap');
html, body, [class*="css"]  { font-family: 'IBM Plex Sans KR', sans-serif; }
.stApp { background-color: #F0F2F6; }
[data-testid="stSidebar"] { background-color: #FFFFFF; border-right: 1px solid #E0E0E0; }
.stButton > button {
    border: 1px solid #E0E0E0; border-radius: 10px; color: #31333F;
    background-color: #FFFFFF; transition: all 0.2s ease-in-out;
    box-shadow: 0 1px 3px rgba(0,0,0,0.05);
}
.stButton > button:hover {
    border-color: #0068C9; color: #0068C9;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
}
div[data-testid="stSidebar"] .stButton > button {
    background-color: #0068C9; color: white; border: none;
}
div[data-testid="stSidebar"] .stButton > button:hover {
    background-color: #0055A3; color: white;
}
</style>
""", unsafe_allow_html=True)

# --- 2. ë°±ì—”ë“œ ë¡œì§: RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • (ì˜¤ë¥˜ ë°©ì§€ ê°•í™”) ---

# ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ì„¤ì •
DB_PATH = "./vector_db"
DATA_PATH = "./data"

def check_api_key():
    """API í‚¤ ìœ ë¬´ë¥¼ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í‘œì‹œ í›„ ì•±ì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
    except (KeyError, FileNotFoundError):
        st.error("ì˜¤ë¥˜: Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.info("Streamlit Cloud ë°°í¬ ì‹œ, ì•± ì„¤ì •ì˜ 'Secrets'ì— GOOGLE_API_KEYë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop() # [ê°œì„ ] API í‚¤ê°€ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ì„ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ì—¬ í˜¼ë€ ë°©ì§€

def create_and_load_db(embedding_function):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ë¡œë“œí•˜ëŠ” í•¨ìˆ˜."""
    st.sidebar.info("ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    
    # [ê°œì„ ] ë°ì´í„° í´ë” ë° íŒŒì¼ ìœ ë¬´ ëª…ì‹œì  í™•ì¸
    if not os.path.exists(DATA_PATH) or not any(f.endswith('.pdf') for f in os.listdir(DATA_PATH)):
        st.error(f"ì˜¤ë¥˜: '{DATA_PATH}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í´ë” ë‚´ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.info("ë°°í¬ ì‹œ, GitHub ë¦¬í¬ì§€í† ë¦¬ì— 'data' í´ë”ì™€ ê·¸ ì•ˆì— PDF íŒŒì¼ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()

    documents = []
    for file in os.listdir(DATA_PATH):
        if file.endswith('.pdf'):
            loader = PyPDFLoader(os.path.join(DATA_PATH, file))
            documents.extend(loader.load())
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(documents=texts, embedding=embedding_function, persist_directory=DB_PATH)
    st.sidebar.success("ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    return vectorstore

@st.cache_resource
def get_rag_pipeline():
    """
    RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì„¤ì •í•˜ê³  ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë¶€ë¶„ë“¤ì„ ëª…í™•íˆ ë¶„ë¦¬í•˜ê³  í™•ì¸í•©ë‹ˆë‹¤.
    """
    # 1. API í‚¤ í™•ì¸ (ê°€ì¥ ë¨¼ì €)
    check_api_key()

    # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
    if os.path.exists(DB_PATH):
        st.sidebar.info("ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    else:
        vectorstore = create_and_load_db(embeddings)

    retriever = vectorstore.as_retriever()

    # 4. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.1)
    
    template = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì •ë¶€ ì •ì±… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ì˜ 'ë¬¸ì„œ ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ, ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    [ë¬¸ì„œ ë‚´ìš©]
    {context}
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {question}
    [ë‹µë³€]
    """
    prompt_template = PromptTemplate.from_template(template)

    # 5. RAG ì²´ì¸ êµ¬ì„± (ê°„ì†Œí™”ëœ í‘œí˜„)
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt_template
        | llm
        | StrOutputParser()
    )
    
    return rag_chain, retriever

# --- 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ---

# RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ
try:
    rag_chain, retriever = get_rag_pipeline()
except Exception as e:
    st.error(f"RAG íŒŒì´í”„ë¼ì¸ì„ ì„¤ì •í•˜ëŠ” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "profile" not in st.session_state:
    st.session_state.profile = {}
if "selected_question" not in st.session_state:
    st.session_state.selected_question = None

# ì¢Œì¸¡ ì‚¬ì´ë“œë°” UI
with st.sidebar:
    st.header("ğŸ¯ ë‚˜ì˜ ë§ì¶¤ ì¡°ê±´ ì„¤ì •")
    st.markdown("AIê°€ ë” ì •í™•í•œ ì •ì±…ì„ ì¶”ì²œí•˜ë„ë¡ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    age = st.number_input("ë‚˜ì´(ë§Œ)", min_value=18, max_value=100, value=st.session_state.profile.get("age", 25))
    interests = st.multiselect(
        "ì£¼ìš” ê´€ì‹¬ ë¶„ì•¼",
        ['ì£¼ê±° ì§€ì›', 'ì¼ìë¦¬/ì°½ì—…', 'ê¸ˆìœµ/ìì‚° í˜•ì„±', 'ìƒí™œ/ë³µì§€'],
        default=st.session_state.profile.get("interests", [])
    )
    if st.button("âœ… ì¡°ê±´ ì €ì¥ ë° ë°˜ì˜", type="primary", use_container_width=True):
        st.session_state.profile = { "age": age, "interests": interests }
        st.success("ë§ì¶¤ ì¡°ê±´ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        time.sleep(1)
        st.rerun()

# ë©”ì¸ í™”ë©´ UI
st.title("ğŸ¤– ì •ì±… íë ˆì´í„°")
st.caption("AI ê¸°ë°˜ ë§ì¶¤í˜• ì •ì±… íƒìƒ‰ê¸°")

# ì¶”ì²œ ì§ˆë¬¸
recommended_questions_db = {
    "ì£¼ê±° ì§€ì›": ["ì²­ë…„ ì›”ì„¸ ì§€ì› ìê²© ì•Œë ¤ì¤˜", "ì‹ í˜¼ë¶€ë¶€ ì „ì„¸ ëŒ€ì¶œ ì¡°ê±´"],
    "ì¼ìë¦¬/ì°½ì—…": ["ì°½ì—… ì§€ì›ê¸ˆ ì¢…ë¥˜ ì•Œë ¤ì¤˜", "ë‚´ì¼ì±„ì›€ê³µì œ ì‹ ì²­ ë°©ë²•"],
}
st.markdown("##### ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
profile_interests = st.session_state.get("profile", {}).get("interests", [])
questions_to_show = recommended_questions_db.get(profile_interests[0], ["ì²­ë…„ ì›”ì„¸ ì§€ì› ìê²© ì•Œë ¤ì¤˜", "ë‚´ì¼ì±„ì›€ê³µì œ ì‹ ì²­ ë°©ë²•"]) if profile_interests else ["ì²­ë…„ ì›”ì„¸ ì§€ì› ìê²© ì•Œë ¤ì¤˜", "ë‚´ì¼ì±„ì›€ê³µì œ ì‹ ì²­ ë°©ë²•"]
cols = st.columns(len(questions_to_show))
for i, question in enumerate(questions_to_show):
    if cols[i].button(question, use_container_width=True, key=f"rec_q_{i}"):
        st.session_state.selected_question = question

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë¡œì§

# ë™ì  ì˜¨ë³´ë”© ë©”ì‹œì§€
if not st.session_state.messages:
    profile = st.session_state.get("profile", {})
    welcome_message = f"ì•ˆë…•í•˜ì„¸ìš”! {profile['age']}ì„¸, '{profile['interests'][0]}' ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆìœ¼ì‹œêµ°ìš”." if profile.get("age") and profile.get("interests") else "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
    st.session_state.messages.append({"role": "assistant", "content": welcome_message})

# ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message:
            with st.expander("ğŸ“š ê·¼ê±° ìë£Œ í™•ì¸í•˜ê¸°"):
                for source in message["sources"]:
                    st.info(f"ì¶œì²˜: {source.metadata.get('source', 'N/A')} (í˜ì´ì§€: {source.metadata.get('page', 'N/A')})")
                    st.write(source.page_content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
prompt = st.chat_input("ê¶ê¸ˆí•œ ì •ì±…ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
if st.session_state.selected_question:
    prompt = st.session_state.selected_question
    st.session_state.selected_question = None

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                response = rag_chain.invoke(prompt)
                sources = retriever.invoke(prompt)
                st.markdown(response)
                st.session_state.messages.append({
                    "role": "assistant", "content": response, "sources": sources
                })
            except Exception as e:
                error_message = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})
    
    st.rerun()
